[{"content":"Jonathan Crussell\n2025 Workshop on Binary Analysis Research (BAR 2025)\nAbstract Malware analysis relies on evolving tools that undergo continuous improvement and refinement. One such tool is Ghidra, released as open-source in 2019, which has seen 39 public releases and 13,000 commits as of October 2024. In this paper, we examine the impact of these updates on code similarity analysis for the same set of input files. Additionally, we measure how the underlying version of Ghidra affects simple metrics such as analysis time, error counts, and the number of functions identified. Our case studies reveal that Ghidra\u0026rsquo;s effectiveness varies depending on the specific file analyzed, highlighting the importance of context in evaluating tool performance.\nWe do not yet have an answer to the question posed in the title of this paper. In general, Ghidra has certainly improved in the years since it was released. Developers have fixed countless bugs, added substantial new features, and supported several new program formats. However, we observe that better is highly nuanced. We encourage the community to approach version upgrades with caution, as the latest release may not always provide superior results for every use case. By fostering a nuanced understanding of Ghidra\u0026rsquo;s advancements, we aim to contribute to more informed decision-making regarding tool adoption and usage in malware analysis and other binary analysis domains.\nCitation @inproceedings{crussell2025ghidra, title={Ghidra: Is Newer Always Better?}, author={Crussell, Jonathan} booktitle={Workshop on Binary Analysis Research (BAR) 2025} year={2025} } Copy Links:\nPDF Project: ghidra-galore ","permalink":"/publication/crussell2025ghidra/","summary":"Examines impact of Ghidra\u0026rsquo;s 39 releases and 13,000 commits on code similarity analysis and metrics like analysis time and function detection, revealing that newer versions don\u0026rsquo;t always provide superior results for every use case.","title":"Ghidra: Is Newer Always Better?"},{"content":"Run binaries through multiple versions of Ghidra via Docker to compare analysis results.\nLinks:\nGitHub Repository Publication: crussell2025ghidra ","permalink":"/project/ghidra-galore/","summary":"Automated Docker framework enabling comparative analysis of binary files across all 39 Ghidra versions to measure how tool updates affect disassembly, decompilation, and code similarity results.","title":"ghidra-galore"},{"content":"Gianluca Geraci, Jonathan Crussell, Laura P. Swiler, Bert J. Debusschere\nInternational Journal for Uncertainty Quantification, Volume 11, Issue 1 (2021)\nAbstract Network modeling is a powerful tool to enable rapid analysis of complex systems that can be challenging to study directly using physical testing. Two approaches are considered: emulation and simulation. The former runs real software on virtualized hardware, while the latter mimics the behavior of network components and their interactions in software. Although emulation provides an accurate representation of physical networks, this approach alone cannot guarantee the characterization of the system under realistic operative conditions. Operative conditions for physical networks are often characterized by intrinsic variability (payload size, packet latency, etc.) or a lack of precise knowledge regarding the network configuration (bandwidth, delays, etc.), therefore Uncertainty Quantification (UQ) strategies should be also employed. UQ strategies require multiple evaluations of the system with a number of evaluation instances that roughly increases with the problem dimensionality, i.e. the number of uncertain parameters. It follows that a typical UQ workflow for network modeling based on emulation can easily become unattainable due to its prohibitive computational cost. In this paper, a multifidelity sampling approach is discussed and applied to network modeling problems. The main idea is to optimally fuse information coming from simulations, which are a low-fidelity version of the emulation problem of interest, in order to decrease the estimator variance. By reducing the estimator variance in a sampling approach it is usually possible to obtain more reliable statistics and therefore a more reliable system characterization. Several network problems of increasing difficulty are presented. For each of them, the performance of the multifidelity estimator is compared with respect to the single fidelity counterpart, namely Monte Carlo sampling. For all the test problems studied in this work, the multifidelity estimator demonstrated an increased efficiency with respect to MC.\nCitation @article{geraci2021exploration, author = {Gianluca Geraci and Jonathan Crussell and Laura P. Swiler and Bert J. Debusschere}, title = {Exploration of Multifidelity Uq Sampling Strategies for Computer Network Applications}, journal = {International Journal for Uncertainty Quantification}, issn = {2152-5080}, year = {2021}, volume = {11}, number = {1}, URL = {https://dl.begellhouse.com/journals/52034eb04b657aea,44206a5570d76285,7d0ea0cc64023c4f.html}, pages = {93--118}, DOI = {10.1615/Int.J.UncertaintyQuantification.2021033774}, } Copy Links:\nPDF Publisher Project: minimega Project: que-ldrd-tools ","permalink":"/publication/geraci2021exploration/","summary":"Journal article demonstrating that multifidelity sampling techniques combining high and low-fidelity network simulations produce estimators with significantly lower variance, making them effective UQ tools for network analysis.","title":"Exploration of multifidelity UQ sampling strategies for computer network applications"},{"content":"Jonathan Crussell, David Fritz, Vince Urias\narXiv preprint arXiv:2003.11143\nAbstract Sandia has an extensive background in cybersecurity research and is currently extending its state-of-the-art modeling via emulation capability. However, a key part of Sandia\u0026rsquo;s modeling methodology is the discovery and specification of the information-system under study, and the ability to recreate that specification with the highest fidelity possible in order to extrapolate meaningful results.\nThis work details a method to conduct information system discovery and develop tools to enable the creation of high-fidelity emulation models that can be used to enable assessment of our infrastructure information system security posture and potential system impacts that could result from cyber threats. The outcome are a set of tools and techniques to go from network discovery of operational systems to emulating complex systems.\nAs a concrete usecase, we have applied these tools and techniques at Supercomputing 2016 to model SCinet, the world’s largest research network. This model includes five routers and nearly 10,000 endpoints which we have launched in our emulation platform.\nCitation @misc{crussell2020automated, title={Automated Discovery for Emulytics}, author={Jonathan Crussell and David Fritz and Vince Urias}, year={2020}, eprint={2003.11143}, archivePrefix={arXiv}, primaryClass={cs.NI}, url={https://arxiv.org/abs/2003.11143}, } Copy Links:\narXiv Project: discovery ","permalink":"/publication/crussell2020automated/","summary":"Describes automated methods and tools for discovering information systems through network and host analysis to create high-fidelity emulation models, demonstrated on SCinet with 5 routers and 10,000 endpoints.","title":"Automated Discovery for Emulytics"},{"content":"Jonathan Crussell, Aaron Brown, Jeremy Kyle Jennings, David Kavaler, Thomas M Kroeger, Cynthia Phillips\nSandia National Laboratories Laboratory Directed Research and Development Report\nAbstract This report summarizes the work performed under the project \u0026ldquo;Quantifying Uncertainty in Emulations.\u0026rdquo; Emulation can be used to model real-world systems, typically using virtualization to run the real software on virtualized hardware. Emulations are increasingly used to answer mission-oriented questions, but how well they represent the real-world systems is still an open area of research. The goal of the project was to quantify where and how emulations differ from the real world. To do so, we ran a representative workload on both, and collected and compared metrics to identify differences. We aimed to capture behaviors, rather than performance, differences as the latter is more well-understood in the literature.\nThis report summarizes the project\u0026rsquo;s major accomplishments, with the background to understand these accomplishments. It gathers the abstracts and references for the refereed publications that have appeared as part of this work. We then archive partial work not yet ready for publication.\nCitation @techreport{crussell2019quantifying, title={Quantifying Uncertainty in Emulations: LDRD Report.}, author={Crussell, Jonathan and Brown, Aaron and Jennings, Jeremy Kyle and Kavaler, David and Kroeger, Thomas M and Phillips, Cynthia A}, year={2019}, institution={Sandia National Lab., Livermore, CA (United States)} } Copy Links:\nPDF Project: minimega Project: que-ldrd-tools ","permalink":"/publication/crussell2019quantifying/","summary":"Sandia LDRD report summarizing a three-year project to quantify behavioral (not performance) differences between emulations and real-world systems by running representative workloads on both and comparing collected metrics.","title":"Quantifying Uncertainty in Emulations: LDRD Report"},{"content":"Tools developed in support of the QUE (Quantifying Uncertainty in Emulations) LDRD project at Sandia National Laboratories. These tools enable systematic comparison of virtual and physical testbeds by executing identical workloads on each environment and analyzing the resulting measurements to quantify performance and behavioral differences.\nThe project supports experimental validation work published in multiple academic papers examining the equivalency of virtual and physical testbeds for cyber security research.\nLinks:\nGitHub Repository Publication: crussell2019quantifying Publication: crussell2019lessons Publication: geraci2019exploration Publication: geraci2021exploration ","permalink":"/project/que-ldrd-tools/","summary":"Research infrastructure that executed over 10,000 experiments processing half a petabyte of data to quantify behavioral differences between virtual and physical testbeds for cyber security research validation.","title":"QUE LDRD Tools"},{"content":"Jonathan Crussell, Thomas M Kroeger, David Kavaler, Aaron Brown, Cynthia Phillips\n12th USENIX Workshop on Cyber Security Experimentation and Test (CSET 2019)\nAbstract Virtual testbeds are a core component of cyber experimentation as they allow for fast and relatively inexpensive modeling of computer systems. Unlike simulations, virtual testbeds run real software on virtual hardware which allows them to capture unknown or complex behaviors. However, virtualization is known to increase latency and decrease throughput. Could these and other artifacts from virtualization undermine the experiments that we wish to run?\nFor the past three years, we have attempted to quantify where and how virtual testbeds differ from their physical counterparts to address this concern. While performance differences have been widely studied, we aim to uncover behavioral differences. We have run over 10,000 experiments and processed over half a petabyte of data. Complete details of our methodology and our experimental results from applying that methodology are published in previous work. In this paper, we describe our lessons learned in the process of constructing and instrumenting both physical and virtual testbeds and analyzing the results from each.\nCitation @inproceedings{crussell2019lessons, title={Lessons learned from 10k experiments to compare virtual and physical testbeds}, author={Crussell, Jonathan and Kroeger, Thomas M and Kavaler, David and Brown, Aaron and Phillips, Cynthia}, booktitle={12th $\\{$USENIX$\\}$ Workshop on Cyber Security Experimentation and Test ($\\{$CSET$\\}$ 19)}, year={2019} } Copy Links:\nPDF Slides Project: minimega Project: que-ldrd-tools ","permalink":"/publication/crussell2019lessons/","summary":"Documents lessons learned from running over 10,000 experiments and processing half a petabyte of data to quantify behavioral (not just performance) differences between virtual and physical testbeds for cyber security research.","title":"Lessons learned from 10k experiments to compare virtual and physical testbeds"},{"content":"Gianluca Geraci, Laura P. Swiler, Jonathan Crussell, Bert J. Debusschere\n3rd International Conference on Uncertainty Quantification in Computational Sciences and Engineering (UNCECOMP 2019)\nAbstract Communication networks have evolved to a level of sophistication that requires computer models and numerical simulations to understand and predict their behavior. A network simulator is a software that enables the network designer to model several components of a computer network such as nodes, routers, switches and links and events such as data transmissions and packet errors in order to obtain device and network level metrics. Network simulations, as many other numerical approximations that model complex systems, are subject to the specification of parameters and operative conditions of the system. Very often the full characterization of the system and their input is not possible, therefore Uncertainty Quantification (UQ) strategies need to be deployed to evaluate the statistics of its response and behavior. UQ techniques, despite the advancements in the last two decades, still suffer in the presence of a large number of uncertain variables and when the regularity of the systems response cannot be guaranteed. In this context, multifidelity approaches have gained popularity in the UQ community recently due to their flexibility and robustness with respect to these challenges. The main idea behind these techniques is to extract information from a limited number of high-fidelity model realizations and complement them with a much larger number of a set of lower fidelity evaluations. The final result is an estimator with a much lower variance, i.e. a more accurate and reliable estimator can be obtained. In this contribution we investigate the possibility to deploy multifidelity UQ strategies to computer network analysis. Two numerical configurations are studied based on a simplified network with one client and one server. Preliminary results for these tests suggest that multifidelity sampling techniques might be used as effective tools for UQ tools in network applications.\nCitation @inproceedings{geraci2019exploration, title={Exploration Of Multifidelity Approaches For Uncertainty Quantification In Network Applications}, author={Geraci, Gianluca and Swiler, Laura P., and Crussell, Jonathan and Debusschere, Bert J.}, booktitle={3rd International Conference on Uncertainty Quantification in Computational Sciences and Engineering}, year={2019} } Copy Links:\nPDF Project: minimega Project: que-ldrd-tools ","permalink":"/publication/geraci2019exploration/","summary":"Investigates multifidelity UQ strategies for network simulations by combining limited high-fidelity model runs with numerous lower-fidelity evaluations to achieve more accurate estimators with lower variance.","title":"Exploration Of Multifidelity Approaches For Uncertainty Quantification In Network Applications"},{"content":"Jonathan Crussell, Thomas M Kroeger, Aaron Brown, Cynthia Phillips\n2019 International Conference on Computing, Networking and Communications (ICNC 2019)\nAbstract Network designers, planners, and security professionals increasingly rely on large-scale testbeds based on virtualization to emulate networks and make decisions about real-world deployments. However, there has been limited research on how well these virtual testbeds match their physical counterparts. Specifically, does the virtualization that these testbeds depend on actually capture real-world behaviors sufficiently well to support decisions?\nAs a first step, we perform simple experiments on both physical and virtual testbeds to begin to understand where and how the testbeds differ. We set up a web service on one host and run ApacheBench against this service from a different host, instrumenting each system during these tests.\nWe define an initial repeatable methodology (algorithm) to quantitatively compare physical and virtual testbeds. Specifically we compare the testbeds at three levels of abstraction: application, operating system (OS) and network. For the application level, we use the ApacheBench results. For OS behavior, we compare patterns of system call orderings using Markov chains. This provides a unique visual representation of the workload and OS behavior in our testbeds. We also drill down into read-system-call behaviors and show how at one level both systems are deterministic and identical, but as we move up in abstractions that consistency declines. Finally, we use packet captures to compare network behaviors and performance. We reconstruct flows and compare per-flow and per-experiment statistics.\nFrom these comparisons, we find that the behavior of the workload in the testbeds is similar but that the underlying processes to support it do vary. The low-level network behavior can vary quite widely in packetization depending on the virtual network driver. While these differences can be important, and knowing about them will help experiment designers, the core application and OS behaviors still represent similar processes.\nCitation @inproceedings{crussell2019virtually, title={Virtually the same: Comparing physical and virtual testbeds}, author={Crussell, Jonathan and Kroeger, Thomas M and Brown, Aaron and Phillips, Cynthia}, booktitle={2019 International Conference on Computing, Networking and Communications (ICNC)}, pages={847--853}, year={2019}, organization={IEEE} } Copy Links:\narXiv Project: minimega ","permalink":"/publication/crussell2019virtually/","summary":"Comparative analysis quantifying behavioral differences between physical and virtual testbeds for cyber security research to assess the fidelity of virtualized environments for experimentation.","title":"Virtually the same: Comparing physical and virtual testbeds"},{"content":"The discovery toolset for Emulytics enables the construction of high-fidelity emulation models of systems. The toolset consists of a set of tools and techniques to automatically go from network discovery of operational systems to emulating those complex systems. Our toolset combines data from host discovery and network mapping tools into an intermediate representation that can then be further refined. Once the intermediate representation reaches the desired state, our toolset supports emitting the Emulytics models with varying levels of specificity based on experiment needs.\nLinks:\nGitHub Repository Publication: crussell2020automated ","permalink":"/project/discovery/","summary":"Automated toolset that transforms network and host discovery data into high-fidelity emulation models, demonstrated by modeling SCinet\u0026rsquo;s 10,000 endpoints and enabling rapid Emulytics experimentation.","title":"discovery toolset"},{"content":"minimega is a tool for launching and managing virtual machines. It can run on your laptop or distributed across a cluster. minimega is fast, easy to deploy, and can scale to run on massive clusters with virtually no setup.\nLinks:\nSlides GitHub Repository Project Website Publication: crussell2019quantifying Publication: crussell2019lessons Publication: geraci2019exploration Publication: geraci2021exploration Publication: crussell2019virtually ","permalink":"/project/minimega/","summary":"Fast, lightweight distributed VM orchestration platform that scales from laptops to massive clusters with virtually no setup, enabling large-scale cyber security experiments and emulation testbeds.","title":"minimega toolset"},{"content":"Ryan Stevens, Jonathan Crussell, Hao Chen\nProceedings of the Sixth ACM Conference on Data and Application Security and Privacy (CODASPY 2016)\nAbstract Many mobile services consist of two components: a server providing an API, and an application running on smartphones and communicating with the API. An unresolved problem in this design is that it is difficult for the server to authenticate which app is accessing the API. This causes many security problems. For example, the provider of a private network API has to embed secrets in its official app to ensure that only this app can access the API; however, attackers can uncover the secret by reverse-engineering. As another example, malicious apps may send automatic requests to ad servers to commit ad fraud.\nIn this work, we propose a system that allows network API to authenticate the mobile app that sends each request so that the API can make an informed access control decision. Our system, the Mobile Trusted-Origin Policy, consists of two parts: 1) an app provenance mechanism that annotates outgoing HTTP(S) requests with information about which app generated the network traffic, and 2) a code isolation mechanism that separates code within an app that should have different app provenance signatures into mobile origin. As motivation for our work, we present two previously-unknown families of apps that perform click fraud, and examine how the lack of mobile origin information enables the attacks. Based on our observations, we propose Trusted Cross-Origin Requests to handle point (1), which automatically includes mobile origin information in outgoing HTTP requests. Servers may then decide, based on the mobile origin data, whether to process the request or not. We implement a prototype of our system for Android and evaluate its performance, security, and deployability. We find that our system can achieve our security and utility goals with negligible overhead.\nCitation @inproceedings{stevens2016origin, title={On the origin of mobile apps: Network provenance for android applications}, author={Stevens, Ryan and Crussell, Jonathan and Chen, Hao}, booktitle={Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy}, pages={160--171}, year={2016} } Copy Links:\nPDF ","permalink":"/publication/stevens2016origin/","summary":"Proposes Mobile Trusted-Origin Policy to authenticate mobile apps accessing network APIs by annotating HTTP requests with app provenance, preventing click fraud and API abuse through code isolation and origin verification.","title":"On the Origin of Mobile Apps: Network Provenance for Android Applications"},{"content":"Jonathan Crussell, Philip Kegelmeyer\nProceedings of the 2015 SIAM International Conference on Data Mining (SDM 2015)\nAbstract Many security applications depend critically on clustering. However, we do not know of any clustering algorithms that were designed with an adversary in mind. An intelligent adversary may be able to use this to her advantage to subvert the security of the application. Already, adversaries use obfuscation and other techniques to alter the representation of their inputs in feature space to avoid detection. For example, malware is often packed and spam email often mimics normal email. In this work, we investigate a more active attack, in which an adversary attempts to subvert clustering analysis by feeding in carefully crafted data points.\nSpecifically, in this work we explore how an attacker can subvert DBSCAN, a popular density-based clustering algorithm. We explore a \u0026ldquo;confidence attack,\u0026rdquo; where an adversary seeks to poison the clusters to the point that the defender loses confidence in the utility of the system. This may result in the system being abandoned, or worse, waste the defender\u0026rsquo;s time investigating false alarms. While our attacks generalize to all DBSCAN-based tools, we focus our evaluation on AnDarwin, a tool designed to detect plagiarized Android apps. We show that an adversary can merge arbitrary clusters by connecting them with \u0026ldquo;bridges\u0026rdquo;, that even a small number of merges can greatly degrade clustering performance, and that the defender has limited recourse when relying solely on DBSCAN. Finally, we propose a remediation process that uses machine learning and features based on outlier measures that are orthogonal to the underlying clustering problem to detect and remove injected points.\nCitation @inproceedings{crussell2015attacking, title={Attacking dbscan for fun and profit}, author={Crussell, Jonathan and Kegelmeyer, Philip}, booktitle={Proceedings of the 2015 SIAM International Conference on Data Mining}, pages={235--243}, year={2015}, organization={Society for Industrial and Applied Mathematics} } Copy Links:\nPDF ","permalink":"/publication/crussell2015attacking/","summary":"Demonstrates how adversaries can subvert DBSCAN clustering by injecting bridge points to merge arbitrary clusters, degrading system performance, and proposes machine learning-based remediation using outlier detection.","title":"Attacking DBSCAN for Fun and Profit"},{"content":"Jonathan Crussell\nPhD Dissertation, University of California, Davis (2014)\nAbstract Smart phones are rapidly becoming a fixture of modern day life. Their popularity and market penetration have given rise to a flourishing ecosystem of mobile apps that provide users with a wide range of useful functionality. Android users may download apps from Google\u0026rsquo;s official Android Market or from a number of third-party markets. To ensure a healthy mobile app environment, users should have access to high quality apps and developers should be financially compensated for their efforts. However, apps may be copied, or \u0026ldquo;cloned,\u0026rdquo; by a dishonest developer and released as her own, subverting revenue from the original developer or possibly including additional malicious functionality.\nI present two approaches to detect similar Android apps based on semantic information. I implement the first approach in a tool called DNADroid which robustly computes the similarity between two apps by comparing program dependency graphs between methods in candidate apps. The second approach, implemented in a tool called AnDarwin, is capable of detecting similar apps on an unprecedented scale. In contrast to earlier approaches, AnDarwin has four advantages: it avoids comparing apps pairwise, thus greatly improving its scalability; it analyzes only the app code and does not rely on other information — such as the app\u0026rsquo;s market, signature, or description — thus greatly increasing its reliability; it can detect both full and partial app similarity; and it can automatically detect library code and remove it from the similarity analysis. I evaluate DNADroid and AnDarwin on many Android apps crawled from multiple Android markets including the official Android Market. My evaluation demonstrates these tools\u0026rsquo; ability to accurately detect similar apps. Finally, I show how DNADroid and AnDarwin can be used in conjunction with other tools to gain insights into the app ecosystem such as the prevalence of malware families that commit ad fraud.\nCitation @phdthesis{crussell2014scalable, title={Scalable Semantics-Based Detection of Similar Android Apps: Design, Implementation, and Applications}, author={Crussell, Jonathan}, year={2014}, school={University of California, Davis} } Copy ","permalink":"/publication/crussell2014scalable/","summary":"PhD dissertation presenting scalable semantics-based approaches for detecting similar Android applications, with applications to clone detection, malware analysis, and security assessment.","title":"Scalable Semantics-Based Detection of Similar Android Apps: Design, Implementation, and Applications"},{"content":"Jonathan Crussell, Clint Gibler, Hao Chen\nIEEE Transactions on Mobile Computing\nAbstract As of March 2012, Android has a majority smart phone marketshare in the United States [15]. The Android operating system provides the core smart phone experience, but much of the user experience relies on third-party apps. To this end, Android has an official market and numerous third-party markets where users can download apps for social networking, games, and more. In order to incentivize developers to continue creating apps, it is important to maintain a healthy market ecosystem.\nOne important aspect of a healthy market ecosystem is that developers are financially compensated for their work. Developers can charge directly for their apps, but many choose instead to offer free apps that are ad-supported or contain in-app billing for additional content. There are several ways developers may lose potential revenue: a paid app may be \u0026ldquo;cracked\u0026rdquo; and released for free or a free app may be copied, or \u0026ldquo;cloned\u0026rdquo;, and re-released with changes to the ad libraries that cause ad revenue to go to the plagiarist [19]. App cloning has been widely reported by developers, smart phone security companies and the academic community [8], [10], [11], [16], [20], [31], [32]. Unfortunately, the openness of Android markets and the ease of repackaging apps contribute to the ability of plagiarists to clone apps and resubmit them to markets.\nAnother aspect of a healthy market ecosystem is the absence of low-quality spam apps which may pollute search results, detracting from hard-working developers. Of the 569,000 apps available on the official Android market, 23 percent are low-quality [7]. Oftentimes, spammers will submit the same app with minor changes as many different apps using one or more developer accounts.\nTo improve the health of the market ecosystem, a scalable approach is needed to detect similar app for use in finding clones and potential spam. As of November, 2012, there are over 569,000 Android apps on the official Android market. Including third-party markets and allowing for future growth, there are too many apps to be analyzed using existing tools.\nTo this end, we develop an approach for detecting similar apps on a unprecedented scale and implement it in a tool called AnDarwin. Unlike previous approaches that compare apps pair-wise, our approach uses multiple clusterings to handle large numbers of apps efficiently. Our efficiency allows us to avoid the need to pre-select potentially similar apps based on their market, name, or description, thus greatly increasing the detection reliability. Additionally, we can use the app clusters produced by AnDarwin to detect when apps have had similar code injected (e.g., the insertion of malware). We investigate two applications of AnDarwin: finding similar apps by different developers (cloned apps) and groups of apps by the same developer with high code reuse (rebranded apps). We demonstrate the utility of AnDarwin, including the detection of new variants of known malware and the detection of new malware.\nCitation @article{crussell2014andarwin, title={Andarwin: Scalable detection of android application clones based on semantics}, author={Crussell, Jonathan and Gibler, Clint and Chen, Hao}, journal={IEEE Transactions on Mobile Computing}, volume={14}, number={10}, pages={2007--2019}, year={2014}, publisher={IEEE} } Copy Links:\nPDF ","permalink":"/publication/crussell2014andarwin/","summary":"Journal article presenting AnDarwin\u0026rsquo;s scalable semantic analysis approach that detected 4,295 cloned and 36,106 rebranded apps from 265,359 applications across 17 markets including Google Play, with automatic library code removal.","title":"Andarwin: Scalable detection of android application clones based on semantics"},{"content":"Jonathan Crussell, Ryan Stevens, Hao Chen\nProceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys 2014)\nAbstract Many Android applications are distributed for free but are supported by advertisements. Ad libraries embedded in the app fetch content from the ad provider and display it on the app\u0026rsquo;s user interface. The ad provider pays the developer for the ads displayed to the user and ads clicked by the user. A major threat to this ecosystem is ad fraud, where a miscreant\u0026rsquo;s code fetches ads without displaying them to the user or \u0026ldquo;clicks\u0026rdquo; on ads automatically. Ad fraud has been extensively studied in the context of web advertising but has gone largely unstudied in the context of mobile advertising.\nWe take the first step to study mobile ad fraud perpetrated by Android apps. We identify two fraudulent ad behaviors in apps: 1) requesting ads while the app is in the background, and 2) clicking on ads without user interaction. Based on these observations, we developed an analysis tool, MAdFraud, which automatically runs many apps simultaneously in emulators to trigger and expose ad fraud. Since the formats of ad impressions and clicks vary widely between different ad providers, we develop a novel approach for automatically identifying ad impressions and clicks in three steps: building HTTP request trees, identifying ad request pages using machine learning, and detecting clicks in HTTP request trees using heuristics. We apply our methodology and tool to two datasets: 1) 130,339 apps crawled from 19 Android markets including Play and many third-party markets, and 2) 35,087 apps that likely contain malware provided by a security company. From analyzing these datasets, we find that about 30% of apps with ads make ad requests while in running in the background. In addition, we find 27 apps which generate clicks without user interaction. We find that the click fraud apps attempt to remain stealthy when fabricating ad traffic by only periodically sending clicks and changing which ad provider is being targeted between installations.\nCitation @inproceedings{crussell2014madfraud, title={Madfraud: Investigating ad fraud in android applications}, author={Crussell, Jonathan and Stevens, Ryan and Chen, Hao}, booktitle={Proceedings of the 12th annual international conference on Mobile systems, applications, and services}, pages={123--134}, year={2014} } Copy Links:\nPDF Slides ","permalink":"/publication/crussell2014madfraud/","summary":"Presents MAdFraud, analyzing 130,339 apps to detect mobile ad fraud including background ad requests (30% of apps) and 27 apps with automated clicking, revealing stealthy techniques to evade detection.","title":"Madfraud: Investigating Ad Fraud in Android Applications"},{"content":"Jonathan Crussell, Clint Gibler, Hao Chen\nEuropean Symposium on Research in Computer Security (ESORICS 2013)\nAbstract The popularity and utility of smartphones rely on their vibrant application markets; however, plagiarism threatens the long-term health of these markets. We present a scalable approach to detecting similar Android apps based on their semantic information. We implement our approach in a tool called AnDarwin and evaluate it on 265,359 apps collected from 17 markets including Google Play and numerous third-party markets. In contrast to earlier approaches, AnDarwin has four advantages: it avoids comparing apps pairwise, thus greatly improving its scalability; it analyzes only the app code and does not rely on other information — such as the app\u0026rsquo;s market, signature, or description — thus greatly increasing its reliability; it can detect both full and partial app similarity; and it can automatically detect library code and remove it from the similarity analysis. We present two use cases for AnDarwin: finding similar apps by different developers (\u0026ldquo;clones\u0026rdquo;) and similar apps from the same developer (\u0026ldquo;rebranded\u0026rdquo;). In ten hours, AnDarwin detected at least 4,295 apps that have been the victims of cloning and 36,106 apps that are rebranded. By analyzing the clusters found by AnDarwin, we found 88 new variants of malware and identified 169 malicious apps based on differences in the requested permissions. Our evaluation demonstrates AnDarwin\u0026rsquo;s ability to accurately detect similar apps on a large scale.\nCitation @inproceedings{crussell2013andarwin, title={Andarwin: Scalable detection of semantically similar android applications}, author={Crussell, Jonathan and Gibler, Clint and Chen, Hao}, booktitle={European Symposium on Research in Computer Security}, pages={182--199}, year={2013}, organization={Springer, Berlin, Heidelberg} } Copy Links:\nPDF ","permalink":"/publication/crussell2013andarwin/","summary":"Introduces AnDarwin, a scalable tool that analyzed 265,359 apps from 17 markets to detect 4,295 clones and 36,106 rebranded apps, discovering 88 new malware variants by comparing semantic information without pairwise comparison.","title":"Andarwin: Scalable detection of semantically similar android applications"},{"content":"Clint Gibler, Ryan Stevens, Jonathan Crussell, Hao Chen, Hui Zang, Heesook Choi\nProceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys 2013)\nAbstract Malicious activities involving Android applications are rising rapidly. As prior work on cyber-crimes suggests, we need to understand the economic incentives of the criminals to design the most effective defenses. In this paper, we investigate application plagiarism on Android markets at a large scale. We take the first step to characterize plagiarized applications and estimate their impact on the original application developers. We first crawled 265,359 free applications from 17 Android markets around the world and ran a tool to identify similar applications (\u0026ldquo;clones\u0026rdquo;). Based on the data, we examined properties of the cloned applications, including their distribution across different markets, application categories, and ad libraries. Next, we examined how cloned applications affect the original developers. We captured HTTP advertising traffic generated by mobile applications at a tier-1 US cellular carrier for 12 days. To associate each Android application with its advertising traffic, we extracted a unique advertising identifier (called the client ID) from both the applications and the network traces. We estimate a lower bound on the advertising revenue that cloned applications siphon from the original developers, and the user base that cloned applications divert from the original applications. To the best of our knowledge, this is the first large scale study on the characteristics of cloned mobile applications and their impact on the original developers.\nCitation @inproceedings{gibler2013adrob, title={Adrob: Examining the landscape and impact of android application plagiarism}, author={Gibler, Clint and Stevens, Ryan and Crussell, Jonathan and Chen, Hao and Zang, Hui and Choi, Heesook}, booktitle={Proceeding of the 11th annual international conference on Mobile systems, applications, and services}, pages={431--444}, year={2013} } Copy Links:\nPDF ","permalink":"/publication/gibler2013adrob/","summary":"First large-scale study characterizing plagiarized Android apps across 265,359 applications from 17 markets and estimating their economic impact by analyzing advertising revenue siphoned from original developers using tier-1 cellular network traces.","title":"Adrob: Examining the Landscape and Impact of Android Application Plagiarism"},{"content":"Jonathan Crussell\nMaster\u0026rsquo;s Thesis, University of California, Davis (2012)\nAbstract Smart phones are rapidly becoming a fixture of modern day life. Their popularity and market penetration have given rise to a flourishing ecosystem of mobile applications that provide users with a wide range of useful functionality. Android users may download applications from Google\u0026rsquo;s official Android Market or from a number of third-party markets. To ensure a healthy mobile application environment, users should have access to high quality applications and developers should be financially compensated for their efforts. However, applications may be copied, or \u0026ldquo;cloned,\u0026rdquo; by a dishonest developer and released as her own, subverting revenue from the original developer or possibly including additional malicious functionality.\nWe present DNADroid, a tool that detects Android application copying, or \u0026ldquo;cloning\u0026rdquo;, by robustly computing the similarity between two applications. DNADroid achieves this by comparing program dependency graphs between methods in candidate applications. Using DNADroid, we found at least 141 applications that have been the victims of cloning, some as many as seven times. DNADroid has a very low false positive rate — we manually confirmed that all the applications detected are indeed clones by either visual or behavioral similarity. We present several case studies that give insight into why applications are cloned, including localization and redirecting ad revenue. We describe a case of malware being added to an application and show how DNADroid was able to detect two variants of the same malware. Lastly, we offer examples of a cracking tool being used in the wild.\nCitation @thesis{crussell2012investigation, title={An Investigation of Android Application Plagiarism}, author={Crussell, Jonathan}, year={2012}, school={University of California, Davis} } Copy ","permalink":"/publication/crussell2012investigation/","summary":"Master\u0026rsquo;s thesis investigating Android application plagiarism through program analysis techniques to detect cloned and repackaged mobile apps.","title":"An Investigation of Android Application Plagiarism"},{"content":"Jonathan Crussell, Clint Gibler, Hao Chen\nEuropean Symposium on Research in Computer Security (ESORICS 2012)\nAbstract We present DNADroid, a tool that detects Android application copying, or \u0026ldquo;cloning\u0026rdquo;, by robustly computing the similarity between two applications. DNADroid achieves this by comparing program dependency graphs between methods in candidate applications. Using DNADroid, we found at least 141 applications that have been the victims of cloning, some as many as seven times. DNADroid has a very low false positive rate — we manually confirmed that all the applications detected are indeed clones by either visual or behavioral similarity. We present several case studies that give insight into why applications are cloned, including localization and redirecting ad revenue. We describe a case of malware being added to an application and show how DNADroid was able to detect two variants of the same malware. Lastly, we offer examples of an open source cracking tool being used in the wild.\nCitation @inproceedings{crussell2012attack, title={Attack of the clones: Detecting cloned applications on android markets}, author={Crussell, Jonathan and Gibler, Clint and Chen, Hao}, booktitle={European Symposium on Research in Computer Security}, pages={37--54}, year={2012}, organization={Springer, Berlin, Heidelberg} } Copy Links:\nPDF ","permalink":"/publication/crussell2012attack/","summary":"Presents DNADroid, a tool that detects Android application cloning by comparing program dependency graphs, identifying at least 141 cloned apps including cases of malware injection and ad revenue redirection.","title":"Attack of the clones: Detecting cloned applications on android markets"},{"content":"Clint Gibler, Jonathan Crussell, Jeremy Erickson, Hao Chen\nTrust and Trustworthy Computing (TRUST 2012)\nAbstract As mobile devices become more widespread and powerful, they store more sensitive data, which includes not only users\u0026rsquo; personal information but also the data collected via sensors throughout the day. When mobile applications have access to this growing amount of sensitive information, they may leak it carelessly or maliciously.\nGoogle\u0026rsquo;s Android operating system provides a permissions-based security model that restricts an application’s access to the user’s private data. Each application statically declares the sensitive data and functionality that it requires in a manifest, which is presented to the user upon installation. However, it is not clear to the user how sensitive data is used once the application is installed. To combat this problem, we present AndroidLeaks, a static analysis framework for automatically finding potential leaks of sensitive information in Android applications on a massive scale. AndroidLeaks drastically reduces the number of applications and the number of traces that a security auditor has to verify manually.\nWe evaluate the efficacy of AndroidLeaks on 24,350 Android applications from several Android markets. AndroidLeaks found 57,299 potential privacy leaks in 7,414 Android applications, out of which we have manually verified that 2,342 applications leak private data including phone information, GPS location, WiFi data, and audio recorded with the microphone. AndroidLeaks examined these applications in 30 hours, which indicates that it is capable of scaling to the increasingly large set of available applications.\nCitation @incollection{gibler2012androidleaks, title={AndroidLeaks: automatically detecting potential privacy leaks in android applications on a large scale}, author={Gibler, Clint and Crussell, Jonathan and Erickson, Jeremy and Chen, Hao}, booktitle={Trust and Trustworthy Computing}, pages={291--307}, year={2012}, publisher={Springer Berlin Heidelberg} } Copy Links:\nPDF ","permalink":"/publication/gibler2012androidleaks/","summary":"Presents AndroidLeaks, which analyzed 24,350 Android apps in 30 hours and found 2,342 apps leaking private data including phone info, GPS location, WiFi data, and audio from 57,299 potential privacy leaks detected.","title":"AndroidLeaks: Automatically Detecting Potential Privacy Leaks in Android Applications on a Large Scale"},{"content":"Ryan Stevens, Clint Gibler, Jonathan Crussell, Jeremy Erickson, Hao Chen\nWorkshop on Mobile Security Technologies (MoST 2012)\nAbstract Recent years have witnessed incredible growth in the popularity and prevalence of smart phones. A flourishing mobile application market has evolved to provide users with additional functionality such as interacting with social networks, games, and more. Mobile applications may have a direct purchasing cost or be free but ad-supported. Unlike in-browser ads, the privacy implications of ads in Android applications has not been thoroughly explored. We start by comparing the similarities and differences of in-browser ads and in-app ads. We examine the effect on user privacy of thirteen popular Android ad providers by reviewing their use of permissions. Worryingly, several ad libraries checked for permissions beyond the required and optional ones listed in their documentation, including dangerous permissions like CAMERA, WRITE_CALENDAR and WRITE_CONTACTS. Further, we discover the insecure use of Android\u0026rsquo;s JavaScript extension mechanism in several ad libraries. We identify fields in ad requests for private user information and confirm their presence in network data obtained from a tier-1 network provider. We also show that users can be tracked by a network sniffer across ad providers and by an ad provider across applications. Finally, we discuss several possible solutions to the privacy issues identified above.\nCitation @inproceedings{stevens2012investigating, title={Investigating User Privacy in Android Ad Libraries}, author={Stevens, Ryan and Gibler, Clint and Crussell, Jon and Erickson, Jeremy and Chen, Hao}, booktitle={Workshop on Mobile Security Technologies (MoST)}, year={2012} } Copy Links:\nPDF ","permalink":"/publication/stevens2012investigating/","summary":"Examines privacy implications of 13 Android ad libraries, discovering several checked dangerous permissions beyond documentation (CAMERA, WRITE_CALENDAR, WRITE_CONTACTS) and identifying cross-app user tracking capabilities.","title":"Investigating User Privacy in Android Ad Libraries"}]